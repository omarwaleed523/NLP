{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df770b4",
   "metadata": {},
   "source": [
    "# Sequential Fine-tuning of MARBERT for Arabic NLP Tasks\n",
    "\n",
    "This notebook demonstrates how to fine-tune the [MARBERT model](https://huggingface.co/UBC-NLP/MARBERT) sequentially on three Arabic NLP tasks:\n",
    "1. Dialect detection (Egypt, MSA, Gulf, Magreb, Levant)\n",
    "2. Sarcasm detection (True, False)\n",
    "3. Sentiment classification (Positive, Negative, Neutral)\n",
    "\n",
    "We'll use the same model architecture and tokenizer, fine-tuning on each task sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e5d38",
   "metadata": {},
   "source": [
    "## Install Required Libraries\n",
    "\n",
    "First, let's install the necessary libraries for our fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b965a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2fb402",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Now let's import all the necessary libraries for our fine-tuning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a4c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "from typing import Dict, List, Union\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ec2fc",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data\n",
    "\n",
    "In this section, we'll load our dataset and prepare it for the three different tasks. We'll use the training-data.csv and testing-data.csv files in our workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd08a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('training-data.csv')\n",
    "test_df = pd.read_csv('testing-data.csv')\n",
    "\n",
    "# Display a few samples to understand the data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b2066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset information\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Testing data shape: {test_df.shape}\")\n",
    "print(\"\\nColumns in dataset:\")\n",
    "print(train_df.columns.tolist())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in training data:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# Check distribution of labels for each task\n",
    "print(\"\\nDialect distribution:\")\n",
    "print(train_df['dialect'].value_counts())\n",
    "\n",
    "print(\"\\nSarcasm distribution:\")\n",
    "print(train_df['sarcasm'].value_counts())\n",
    "\n",
    "print(\"\\nSentiment distribution:\")\n",
    "print(train_df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c336e7",
   "metadata": {},
   "source": [
    "## Helper Functions for Preprocessing and Training\n",
    "\n",
    "Let's create helper functions for preprocessing our data, encoding labels, and evaluating our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_encode_data(df: pd.DataFrame, task: str, tokenizer, max_length: int = 128):\n",
    "    \"\"\"Preprocess text data and encode labels for a specific task\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the data\n",
    "        task: One of 'dialect', 'sarcasm', or 'sentiment'\n",
    "        tokenizer: BERT tokenizer\n",
    "        max_length: Maximum sequence length for tokenization\n",
    "        \n",
    "    Returns:\n",
    "        Processed Dataset object with encoded inputs and labels\n",
    "    \"\"\"\n",
    "    # Get label mapping based on the task\n",
    "    if task == 'dialect':\n",
    "        unique_labels = sorted(df['dialect'].unique())\n",
    "        label_column = 'dialect'\n",
    "    elif task == 'sarcasm':\n",
    "        unique_labels = sorted(df['sarcasm'].unique())\n",
    "        label_column = 'sarcasm'\n",
    "    elif task == 'sentiment':\n",
    "        unique_labels = sorted(df['sentiment'].unique())\n",
    "        label_column = 'sentiment'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task: {task}\")\n",
    "    \n",
    "    label_mapping = {label: i for i, label in enumerate(unique_labels)}\n",
    "    print(f\"Label mapping for {task}: {label_mapping}\")\n",
    "    \n",
    "    # Encode labels\n",
    "    labels = [label_mapping[label] for label in df[label_column]]\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset_dict = {\n",
    "        \"text\": df[\"tweet\"].tolist(),\n",
    "        \"label\": labels\n",
    "    }\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "    # Tokenize function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "    \n",
    "    # Tokenize all examples\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    return tokenized_dataset, label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b6dd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics for evaluation\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    f1 = evaluate.load(\"f1\")\n",
    "    precision = evaluate.load(\"precision\")\n",
    "    recall = evaluate.load(\"recall\")\n",
    "    \n",
    "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    # For multiclass, we use macro averaging\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
    "    precision_score = precision.compute(predictions=predictions, references=labels, average=\"macro\")[\"precision\"]\n",
    "    recall_score = recall.compute(predictions=predictions, references=labels, average=\"macro\")[\"recall\"]\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score,\n",
    "        \"f1\": f1_score,\n",
    "        \"precision\": precision_score,\n",
    "        \"recall\": recall_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab36c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(task: str, \n",
    "                 train_dataset, \n",
    "                 eval_dataset, \n",
    "                 model, \n",
    "                 num_labels: int,\n",
    "                 output_dir: str,\n",
    "                 epochs: int = 3,\n",
    "                 batch_size: int = 16):\n",
    "    \"\"\"Fine-tune MARBERT for a specific task\n",
    "    \n",
    "    Args:\n",
    "        task: The name of the task ('dialect', 'sarcasm', or 'sentiment')\n",
    "        train_dataset: Training dataset\n",
    "        eval_dataset: Evaluation dataset\n",
    "        model: Pre-trained or previously fine-tuned model\n",
    "        num_labels: Number of labels for the task\n",
    "        output_dir: Directory to save the model\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Training batch size\n",
    "        \n",
    "    Returns:\n",
    "        Fine-tuned model\n",
    "    \"\"\"\n",
    "    logging.info(f\"Fine-tuning model for task: {task}\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set up training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        report_to=\"none\",  # Disable reporting to avoid wandb or other integrations\n",
    "    )\n",
    "    \n",
    "    # Set up trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    eval_results = trainer.evaluate()\n",
    "    logging.info(f\"Evaluation results for {task}: {eval_results}\")\n",
    "    \n",
    "    # Save the model\n",
    "    trainer.save_model(output_dir)\n",
    "    logging.info(f\"Model saved to {output_dir}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a1aee8",
   "metadata": {},
   "source": [
    "## Sequential Fine-tuning Pipeline\n",
    "\n",
    "Now we'll implement the sequential fine-tuning pipeline where we'll first fine-tune on dialect detection, then use that model for sarcasm detection, and finally for sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50130ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for saving models\n",
    "dialect_model_path = \"marbert_dialect\"\n",
    "sarcasm_model_path = \"marbert_sarcasm\"\n",
    "sentiment_model_path = \"marbert_sentiment\"\n",
    "\n",
    "# Define hyperparameters\n",
    "model_name = \"UBC-NLP/MARBERT\"\n",
    "max_length = 128\n",
    "batch_size = 16\n",
    "epochs = 3\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a3d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b9c607",
   "metadata": {},
   "source": [
    "### Task 1: Dialect Detection\n",
    "\n",
    "First, we'll fine-tune the MARBERT model on the dialect detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b53547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data for dialect detection task\n",
    "train_dialect_dataset, dialect_label_mapping = preprocess_and_encode_data(\n",
    "    train_df, 'dialect', tokenizer, max_length\n",
    ")\n",
    "eval_dialect_dataset, _ = preprocess_and_encode_data(\n",
    "    test_df, 'dialect', tokenizer, max_length\n",
    ")\n",
    "\n",
    "# Load base MARBERT model for dialect detection\n",
    "dialect_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=len(dialect_label_mapping),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Fine-tune for dialect detection\n",
    "dialect_model = fine_tune_model(\n",
    "    task='dialect',\n",
    "    train_dataset=train_dialect_dataset,\n",
    "    eval_dataset=eval_dialect_dataset,\n",
    "    model=dialect_model,\n",
    "    num_labels=len(dialect_label_mapping),\n",
    "    output_dir=dialect_model_path,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddabf51",
   "metadata": {},
   "source": [
    "### Task 2: Sarcasm Detection\n",
    "\n",
    "Next, we'll use the fine-tuned dialect model as a starting point for the sarcasm detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272ace9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data for sarcasm detection task\n",
    "train_sarcasm_dataset, sarcasm_label_mapping = preprocess_and_encode_data(\n",
    "    train_df, 'sarcasm', tokenizer, max_length\n",
    ")\n",
    "eval_sarcasm_dataset, _ = preprocess_and_encode_data(\n",
    "    test_df, 'sarcasm', tokenizer, max_length\n",
    ")\n",
    "\n",
    "# Load fine-tuned dialect model for sarcasm detection\n",
    "sarcasm_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    dialect_model_path,\n",
    "    num_labels=len(sarcasm_label_mapping),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Fine-tune for sarcasm detection\n",
    "sarcasm_model = fine_tune_model(\n",
    "    task='sarcasm',\n",
    "    train_dataset=train_sarcasm_dataset,\n",
    "    eval_dataset=eval_sarcasm_dataset,\n",
    "    model=sarcasm_model,\n",
    "    num_labels=len(sarcasm_label_mapping),\n",
    "    output_dir=sarcasm_model_path,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f5431",
   "metadata": {},
   "source": [
    "### Task 3: Sentiment Classification\n",
    "\n",
    "Finally, we'll use the fine-tuned sarcasm model as a starting point for the sentiment classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b3ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data for sentiment classification task\n",
    "train_sentiment_dataset, sentiment_label_mapping = preprocess_and_encode_data(\n",
    "    train_df, 'sentiment', tokenizer, max_length\n",
    ")\n",
    "eval_sentiment_dataset, _ = preprocess_and_encode_data(\n",
    "    test_df, 'sentiment', tokenizer, max_length\n",
    ")\n",
    "\n",
    "# Load fine-tuned sarcasm model for sentiment classification\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    sarcasm_model_path,\n",
    "    num_labels=len(sentiment_label_mapping),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Fine-tune for sentiment classification\n",
    "sentiment_model = fine_tune_model(\n",
    "    task='sentiment',\n",
    "    train_dataset=train_sentiment_dataset,\n",
    "    eval_dataset=eval_sentiment_dataset,\n",
    "    model=sentiment_model,\n",
    "    num_labels=len(sentiment_label_mapping),\n",
    "    output_dir=sentiment_model_path,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5191dadb",
   "metadata": {},
   "source": [
    "## Model Inference\n",
    "\n",
    "Now let's create functions to use our fine-tuned models for inference on new Arabic text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c00c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_model(text, model_path, label_mapping):\n",
    "    \"\"\"Make a prediction using a fine-tuned model\n",
    "    \n",
    "    Args:\n",
    "        text: The input Arabic text\n",
    "        model_path: Path to the fine-tuned model\n",
    "        label_mapping: Dictionary mapping numerical labels to text labels\n",
    "        \n",
    "    Returns:\n",
    "        Predicted label and confidence score\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128, padding=\"max_length\")\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "        prediction = torch.argmax(probabilities, dim=1).item()\n",
    "        confidence = probabilities[0][prediction].item()\n",
    "    \n",
    "    # Map numerical label back to text label\n",
    "    inverse_mapping = {v: k for k, v in label_mapping.items()}\n",
    "    predicted_label = inverse_mapping[prediction]\n",
    "    \n",
    "    return predicted_label, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f348f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use the fine-tuned models for inference\n",
    "def analyze_arabic_text(text):\n",
    "    \"\"\"Analyze Arabic text using all three fine-tuned models\n",
    "    \n",
    "    Args:\n",
    "        text: Arabic text input\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with predictions for all three tasks\n",
    "    \"\"\"\n",
    "    # Predict dialect\n",
    "    dialect, dialect_confidence = predict_with_model(\n",
    "        text, dialect_model_path, dialect_label_mapping\n",
    "    )\n",
    "    \n",
    "    # Predict sarcasm\n",
    "    sarcasm, sarcasm_confidence = predict_with_model(\n",
    "        text, sarcasm_model_path, sarcasm_label_mapping\n",
    "    )\n",
    "    \n",
    "    # Predict sentiment\n",
    "    sentiment, sentiment_confidence = predict_with_model(\n",
    "        text, sentiment_model_path, sentiment_label_mapping\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"dialect\": {\n",
    "            \"prediction\": dialect,\n",
    "            \"confidence\": f\"{dialect_confidence:.4f}\"\n",
    "        },\n",
    "        \"sarcasm\": {\n",
    "            \"prediction\": sarcasm,\n",
    "            \"confidence\": f\"{sarcasm_confidence:.4f}\"\n",
    "        },\n",
    "        \"sentiment\": {\n",
    "            \"prediction\": sentiment,\n",
    "            \"confidence\": f\"{sentiment_confidence:.4f}\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb205ab",
   "metadata": {},
   "source": [
    "## Test with Example Sentences\n",
    "\n",
    "Let's test our models with a few example Arabic sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96de3fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test examples (add these after training is complete)\n",
    "test_examples = [\n",
    "    \"أنا سعيد جدا بهذا الخبر العظيم\",  # I am very happy with this great news\n",
    "    \"هههههه والله انك مسخرة يا رجل\",   # Hahaha, you're so funny man\n",
    "    \"الطقس حار جدا اليوم في القاهرة\"    # The weather is very hot today in Cairo\n",
    "]\n",
    "\n",
    "# Uncomment to run predictions after training is complete\n",
    "'''\n",
    "for i, example in enumerate(test_examples):\n",
    "    print(f\"\\nExample {i+1}: {example}\")\n",
    "    results = analyze_arabic_text(example)\n",
    "    print(f\"Dialect: {results['dialect']['prediction']} (confidence: {results['dialect']['confidence']})\")\n",
    "    print(f\"Sarcasm: {results['sarcasm']['prediction']} (confidence: {results['sarcasm']['confidence']})\")\n",
    "    print(f\"Sentiment: {results['sentiment']['prediction']} (confidence: {results['sentiment']['confidence']})\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b232978a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to fine-tune the MARBERT model sequentially on three Arabic NLP tasks: dialect detection, sarcasm detection, and sentiment classification. We've shown how to:\n",
    "\n",
    "1. Load and preprocess the data for each task\n",
    "2. Fine-tune the model sequentially, using the model from the previous task as a starting point\n",
    "3. Save the fine-tuned models\n",
    "4. Use the models for inference on new Arabic text\n",
    "\n",
    "This approach allows for knowledge transfer between related tasks, potentially improving performance over training each task from scratch."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
